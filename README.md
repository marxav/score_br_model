# score_br_model

## Goal
* This project is a quick and dirty project aiming at evaluating some large language models (LLMs) able to translate Breton language into French language ('br2fr' task), and vice versa ('fr2br' task).
* To do so, it compares the semantic distance of a translation performed by an LLM with an expected translation (a.k.a. target translation).
* The semantic distance is based on the proximity of OpenAI embeddings.
* Currently, the models that can be tested are: 
  * OpenAI *gpt-3.5-turbo* and *gpt-4-turbo*
  * Google *gemini-1.0-pro*, *gemini-1.5-flash*, *gemini-1.5-pro*

## Requirements
* Ubuntu OS
* An OPENAI_API_KEY (cf. https://platform.openai.com/api-keys)
* A GOOGLE_API_KEY (cf. https://ai.google.dev/gemini-api/docs/api-key?hl=fr)

## Installation
* git clone https://github.com/marxav/score_br_model.git
* cd score_br_model
* python3 -m venv env
* source env/bin/activate
* pip install openai transformers torch sentencepiece pandas ipykernel tabulate
* echo OPENAI_API_KEY=your-secret-key-1 >> .env
* echo GOOGLE_API_KEY=your-secret-key-2 >> .env

## Run
* cd score_br_model
* source env/bin/activate
* python eval.py samples.tsv 


## Results
* The result file will contain something like :

| task   | model            | score       |
|:-------|:-----------------|:------------|
| br2fr  | gemini-1.5-pro   | 0.82 ± 0.17 |
| br2fr  | gpt-4-turbo      | 0.8 ± 0.21  |
| br2fr  | gemini-1.5-flash | 0.75 ± 0.18 |
| br2fr  | gemini-1.0-pro   | 0.65 ± 0.22 |
| br2fr  | gpt-3.5-turbo    | 0.64 ± 0.16 |

| task   | model            | score       |
|:-------|:-----------------|:------------|
| fr2br  | gpt-4-turbo      | 0.68 ± 0.17 |
| fr2br  | gemini-1.5-pro   | 0.67 ± 0.17 |
| fr2br  | gemini-1.5-flash | 0.64 ± 0.16 |
| fr2br  | gemini-1.0-pro   | 0.58 ± 0.12 |
| fr2br  | gpt-3.5-turbo    | 0.53 ± 0.14 |



## More info
* The input file should be a *.tsv file (e.g. [samples.tsv](samples.tsv)). 
  * The *.tsv must contain two columns named 'br' and 'fr' 
  * Each 'br' or 'fr' sentence must finish with a '.' (i.e. current limitation).  
  * The columns must be separated by a tab (i.e. '\t').  
* Alternatively, the input file may be a *_br.txt file containing only breton sentences (e.g. [tregor_2110_br.txt](tregor_2110_br.txt)). 
  * In this case, another file *_fr.txt file containing the corresponding french translations must be available in the same repository and must contain exactly the same number of sentences than the *_br.tsv file (e.g. [tregor_2110_fr.txt](tregor_2110_fr.txt))
  * A derived *.tsv file resulting from the merge of *_br.txt and *_fr.txt will be automatically generated by our software.
* Running the eval.py creates 2 files 
  * A log file containing all translations and scores;
    * For example: [samples_logs.tsv](samples_logs.tsv)
  * A result file containing the summary of scores.  
    * For example: [samples_res.tsv](samples_res.tsv)
  * To better view these 2 output files, you may use a jupyter notebook.
    * For example: [samples_logs_and_results.ipynb](samples_logs_and_results.ipynb).
  
## Todo
* Enhance the scoring metric(s)
* Add more samples in samples.tsv
* Add more LLMs
* Add a leaderboard of the tested LLMs and theirs scores at different tasks

## Warning
* Currently, we use a file with lines like "br:port nawak" and "fr:trop chouette"
* For the br2fr task "br:port nawak" is sent to the model; the answer is then compared with "fr:trop chouette", which is used as true value.
* For the fr2br task "fr:trop chouette" is sent to the model; the answer is then compared with "br:port nawak", which is used as true value.
* A model saving history could then learn that "br:port nawak" has to be translated by fr:trop chouette", which would bias the evaluation.
* TODO: try to check if this happens... or avoid using the same input-file for the two different tasks.

## Acknowledgments
* [tregor_2110_br.txt](tregor_2110_br.txt) is a sample of a text written by Gireg Konan in Le Tregor newspaper, n°2110, June 6th 2024.
